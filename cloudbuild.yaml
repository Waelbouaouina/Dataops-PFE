# cloudbuild.yaml

options:
  logging: CLOUD_LOGGING_ONLY

substitutions:
  _PROJECT_ID:           "tmtrackdev01"
  _REGION:               "europe-west1"
  _COMPOSER_BUCKET:      "europe-west1-dataops-pfe-composer-env-bucket"
  _DATA_BUCKET:          "tmt-storage-02"
  _TF_VAR_FILE:          "terraform.tfvars"
  _TF_DIR:               "Terraform"
  _BQ_DATASET:           "inventory_dataset"
  _IMAGE:                "gcr.io/$_PROJECT_ID/dataloader-image:latest"
  _FUNCTION_NAME:        "csv-validator"
  _FUNCTION_RUNTIME:     "python39"
  _FUNCTION_ENTRY:       "validate_csv"
  _SUCCESS_TOPIC:        "projects/$_PROJECT_ID/topics/csv-success-topic"
  _ERROR_TOPIC:          "projects/$_PROJECT_ID/topics/csv-error-topic"

steps:
# 0) Enable required APIs
- name: gcr.io/google.com/cloudsdktool/cloud-sdk
  id: Enable APIs
  entrypoint: bash
  args:
    - -c
    - |
      gcloud services enable \
        cloudbuild.googleapis.com \
        pubsub.googleapis.com \
        bigquery.googleapis.com \
        storage.googleapis.com \
        cloudfunctions.googleapis.com \
        composer.googleapis.com \
        run.googleapis.com

# 1) Git Config
- name: gcr.io/google.com/cloudsdktool/cloud-sdk
  id: Git Config
  entrypoint: bash
  args:
    - -c
    - git config --global init.defaultBranch main

# 2) Terraform Init
- name: hashicorp/terraform:latest
  id: Terraform Init
  dir: ${_TF_DIR}
  args:
    - init
    - -reconfigure
    - -backend-config=prod.tfbackend

# 3) Import existing GCS Bucket
- name: hashicorp/terraform:latest
  id: Terraform Import Bucket
  dir: ${_TF_DIR}
  args:
    - import
    - google_storage_bucket.new_data_bucket
    - tmt-storage-02

# 4) Import existing Composer Environment
- name: hashicorp/terraform:latest
  id: Terraform Import Composer
  dir: ${_TF_DIR}
  args:
    - import
    - google_composer_environment.composer_env
    - projects/${_PROJECT_ID}/locations/${_REGION}/environments/composer-environment

# 5) Import existing BigQuery dataset
- name: hashicorp/terraform:latest
  id: Terraform Import Dataset
  dir: ${_TF_DIR}
  args:
    - import
    - google_bigquery_dataset.inventory_dataset
    - projects/tmtrackdev01/datasets/inventory_dataset

# 6) Import existing Cloud Function
- name: hashicorp/terraform:latest
  id: Terraform Import Function
  dir: ${_TF_DIR}
  args:
    - import
    - google_cloudfunctions_function.csv_validator
    - projects/${_PROJECT_ID}/locations/${_REGION}/functions/${_FUNCTION_NAME}

# 7) Terraform IAM Apply (Storage, BigQuery, ActAs)
- name: hashicorp/terraform:latest
  id: Terraform IAM Apply
  dir: ${_TF_DIR}
  args:
    - apply
    - -auto-approve
    - -target=google_project_iam_binding.tf_storage_admin
    - -target=google_project_iam_binding.cb_bq_admin
    - -target=google_service_account_iam_member.cb_actas_dataloader
    - -var-file=${_TF_VAR_FILE}

# 8) Grant Composer Roles
- name: gcr.io/google.com/cloudsdktool/cloud-sdk
  id: Grant Composer Roles
  entrypoint: bash
  args:
    - -c
    - |
      gcloud projects add-iam-policy-binding ${_PROJECT_ID} \
        --member="serviceAccount:tmt-dev-01@${_PROJECT_ID}.iam.gserviceaccount.com" \
        --role="roles/composer.admin"

      gcloud projects add-iam-policy-binding ${_PROJECT_ID} \
        --member="serviceAccount:tmt-dev-01@${_PROJECT_ID}.iam.gserviceaccount.com" \
        --role="roles/composer.worker"

# 9) Terraform Validate
- name: hashicorp/terraform:latest
  id: Terraform Validate
  dir: ${_TF_DIR}
  args:
    - validate

# Ajout du r√¥le Storage Object Viewer au Compute Engine default SA
- name: gcr.io/google.com/cloudsdktool/cloud-sdk
  id: Grant Storage Viewer to Compute SA
  entrypoint: bash
  args:
    - -c
    - |
      gcloud projects add-iam-policy-binding ${_PROJECT_ID} \
        --member="serviceAccount:101312727189-compute@developer.gserviceaccount.com" \
        --role="roles/storage.objectViewer"

# 10) Terraform Apply (rest of infra)
- name: hashicorp/terraform:latest
  id: Terraform Apply All
  dir: ${_TF_DIR}
  args:
    - apply
    - -auto-approve
    - -var-file=${_TF_VAR_FILE}

# 11) Build & Push Docker image
- name: gcr.io/cloud-builders/docker
  id: Build Dataloader
  args:
    - build
    - -t
    - "gcr.io/tmtrackdev01/dataloader-image:latest"
    - -f
    - "Dataloader/Dockerfile"
    - "."

- name: gcr.io/cloud-builders/docker
  id: Push Dataloader
  args:
    - push
    - "gcr.io/tmtrackdev01/dataloader-image:latest"


# 12) Deploy Cloud Run
- name: gcr.io/google.com/cloudsdktool/cloud-sdk
  id: Deploy Cloud Run
  entrypoint: bash
  args:
    - -c
    - |
      gcloud run deploy dataloader-service \
        --image="${_IMAGE}" \
        --region="${_REGION}" \
        --platform=managed \
        --service-account="dataloader-sa@${_PROJECT_ID}.iam.gserviceaccount.com" \
        --allow-unauthenticated

# 13) Deploy Cloud Function
- name: gcr.io/google.com/cloudsdktool/cloud-sdk
  id: Deploy CSV Validator
  entrypoint: bash
  args:
    - -c
    - |
      cd cloud_function
      zip -r ../csv_validator.zip .
      gsutil cp ../csv_validator.zip gs://${_DATA_BUCKET}/
      cd ..
      gcloud functions deploy ${_FUNCTION_NAME} \
        --region=${_REGION} \
        --runtime=${_FUNCTION_RUNTIME} \
        --trigger-resource=${_DATA_BUCKET} \
        --trigger-event=google.storage.object.finalize \
        --source=cloud_function \
        --entry-point=${_FUNCTION_ENTRY} \
        --set-env-vars SUCCESS_TOPIC=${_SUCCESS_TOPIC},ERROR_TOPIC=${_ERROR_TOPIC}

# 14) Upload sample CSV
- name: gcr.io/google.com/cloudsdktool/cloud-sdk
  id: Upload CSV
  entrypoint: bash
  args:
    - -c
    - |
      gsutil cp RAPToR_Azure_Resources_Inventory_02-03-2024.csv gs://${_DATA_BUCKET}/

# 15) Sync DAGs to Composer
- name: gcr.io/google.com/cloudsdktool/cloud-sdk
  id: Sync DAGs
  entrypoint: bash
  args:
    - -c
    - |
      gsutil -m rsync -r Airflow_home/dags gs://${_COMPOSER_BUCKET}/dags
