# cloudbuild.yaml

options:
  # Envoie uniquement les logs vers Cloud Logging, pas besoin de bucket GCS
  logging: CLOUD_LOGGING_ONLY

substitutions:
  _PROJECT_ID:      "tmtrackdev01"
  _REGION:          "europe-west1"
  _COMPOSER_BUCKET: "europe-west1-dataops-pfe-composer-env-bucket"
  _DATA_BUCKET:     "tmt-storage-01"
  # ← On met juste le nom du fichier, pas le chemin relatif
  _TF_VAR_FILE:     "terraform.tfvars"
  _IMAGE:           "gcr.io/$_PROJECT_ID/dataloader-image:latest"

steps:
  # 0) S’assurer que les APIs GCP nécessaires sont activées
  - name: "gcr.io/google.com/cloudsdktool/cloud-sdk"
    id: "Enable APIs"
    entrypoint: "bash"
    args:
      - "-c"
      - |
        gcloud services enable \
          serviceusage.googleapis.com \
          cloudbuild.googleapis.com \
          pubsub.googleapis.com \
          bigquery.googleapis.com \
          dataflow.googleapis.com \
          composer.googleapis.com \
          datacatalog.googleapis.com \
          monitoring.googleapis.com

  # 1) Adoucir le warning Git sur la branche par défaut
  - name: "gcr.io/google.com/cloudsdktool/cloud-sdk"
    id: "Git Config"
    entrypoint: "bash"
    args:
      - "-c"
      - |
        git config --global init.defaultBranch main

  # 2) Terraform Init (télécharge les providers)
  - name: "hashicorp/terraform:light"
    id: "Terraform Init"
    entrypoint: "sh"
    args:
      - "-c"
      - |
        cd Terraform
        terraform init -var-file=$_TF_VAR_FILE

  # 3) Terraform Validate (syntaxe & cohérence)
  - name: "hashicorp/terraform:light"
    id: "Terraform Validate"
    entrypoint: "sh"
    args:
      - "-c"
      - |
        cd Terraform
        terraform validate

  # 4) Terraform Apply (création des ressources)
  - name: "hashicorp/terraform:light"
    id: "Terraform Apply"
    entrypoint: "sh"
    args:
      - "-c"
      - |
        cd Terraform
        terraform apply -auto-approve -var-file=$_TF_VAR_FILE

  # 5) Build + Push Docker image pour Cloud Run
  - name: "gcr.io/cloud-builders/docker"
    id: "Build Dataloader"
    args:
      - "build"
      - "-t"
      - "$_IMAGE"
      - "-f"
      - "Dataloader/Dockerfile"
      - "."
  - name: "gcr.io/cloud-builders/docker"
    id: "Push Dataloader"
    args:
      - "push"
      - "$_IMAGE"

  # 6) Copier le CSV d’inventaire dans Cloud Storage
  - name: "gcr.io/google.com/cloudsdktool/cloud-sdk"
    id: "Upload CSV"
    entrypoint: "bash"
    args:
      - "-c"
      - |
        gsutil cp RAPToR_Azure_Resources_Inventory_02-03-2024.csv gs://$_DATA_BUCKET/

  # 7) Synchroniser les DAGs vers Cloud Composer
  - name: "gcr.io/google.com/cloudsdktool/cloud-sdk"
    id: "Sync DAGs"
    entrypoint: "bash"
    args:
      - "-c"
      - |
        gsutil -m rsync -r Dags gs://$_COMPOSER_BUCKET/dags

  # 8) Déployer / mettre à jour Cloud Run
  - name: "gcr.io/google.com/cloudsdktool/cloud-sdk"
    id: "Deploy Cloud Run"
    entrypoint: "bash"
    args:
      - "-c"
      - |
        gcloud run deploy dataloader-service \
          --image "$_IMAGE" \
          --region "$_REGION" \
          --platform managed \
          --allow-unauthenticated
